{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7458bd4d-0d24-424e-989c-7d16a9a47541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pandas as pd\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import urllib\n",
    "from Data_URL import URL\n",
    "import Scrap_Product_Info_Logic as Scrap\n",
    "\n",
    "# Establish connection to local sql server\n",
    "def connect_to_sql(connection_string):\n",
    "    quoted = urllib.parse.quote_plus(connection_string)\n",
    "\n",
    "    engine = create_engine(\"mssql+pyodbc:///?odbc_connect={}\".format(quoted))\n",
    "    conn = engine.connect()\n",
    "    return conn\n",
    "\n",
    "# Web scraping workflow\n",
    "class ReturnProductData:\n",
    "    def __init__(self, ChromeDriver_PATH, Chrome_option):\n",
    "        self.ChromeDriver_PATH = ChromeDriver_PATH\n",
    "        self.Chrome_option = Chrome_option\n",
    "        self.webdriver = None\n",
    "        self.ResultData = None\n",
    "        self.ProdcutWithUntracableItems = []\n",
    "        self.ProdcutWithUntracableItems_Index = []\n",
    "        self.index_of_product_being_process = None\n",
    "        \n",
    "    def CreateWebdriver(self, URL):\n",
    "        self.webdriver = webdriver.Chrome(options = self.Chrome_option, executable_path = self.ChromeDriver_PATH)\n",
    "        self.webdriver.get(URL)\n",
    "        # Wait for at most 10 seconds for searching the targeted web content \n",
    "        self.webdriver.implicitly_wait(10)\n",
    "    \n",
    "    # Loop over the catalogue for a product category, get each product name and the URL direct to the product detail page\n",
    "    def Get_productAndURL(self, root_url, relative_url):\n",
    "        page_number = 0\n",
    "        x = Scrap.AddProductInfo()\n",
    "        while True:\n",
    "            # Update URL with page number\n",
    "            Full_URL = f'{root_url}+{relative_url}'\n",
    "            page_number_index = re.search(\"page=\", Full_URL)\n",
    "            Full_URL = Full_URL[0:page_number_index.end()] + str(page_number) + Full_URL[page_number_index.end() + 1:]\n",
    "            # Check for whether the targeted web content is avaliable in that URL\n",
    "            try:\n",
    "                self.CreateWebdriver(Full_URL)\n",
    "                self.webdriver.find_element(By.CLASS_NAME, \"product-brief-wrapper\")\n",
    "                # Return the scraped web content\n",
    "                soup = BeautifulSoup(self.webdriver.page_source, 'html.parser')\n",
    "                #Excape the loop if no more products to be captured\n",
    "            except NoSuchElementException:\n",
    "                break\n",
    "            finally:\n",
    "                self.webdriver.quit()\n",
    "            # Extract the product name and product details page URL for each product   \n",
    "            allItems = soup.find_all(\"span\", {'class':\"product-brief-wrapper\"}) \n",
    "            for item in allItems:\n",
    "                try:\n",
    "                    x.addProductName(soup = item)\n",
    "                    x.addProductRelative_URL(soup = item, root_url = root_url)\n",
    "                except AttributeError:\n",
    "                    print(\"Problems in extracting Product Name/ URL for directing to product details\")\n",
    "                    raise\n",
    "            page_number += 1 \n",
    "        # Insert the captured data into the result dataframe\n",
    "        self.ResultData = pd.DataFrame(x.ProductData)\n",
    "        \n",
    "    # Loop over each of the product detail page, extract the supplementary product information and return dataframe with raw product data    \n",
    "    def AppendProductInfo(self):\n",
    "        x = Scrap.AddProductInfo()\n",
    "        for index, row in self.ResultData.iterrows():\n",
    "            self.index_of_product_being_process = index\n",
    "            # Check for whether the targeted content is avaliable in that URL\n",
    "            try:\n",
    "                self.CreateWebdriver(row['Product Relative URL'])\n",
    "                self.webdriver.find_element(By.CLASS_NAME, \"productDetailPage\")\n",
    "                # Return the scraped web content\n",
    "                soup_eachProduct = BeautifulSoup(self.webdriver.page_source, 'html.parser')\n",
    "            except NoSuchElementException:\n",
    "                self.ProdcutWithUntracableItems.append(row['Product Name'])\n",
    "                self.ProdcutWithUntracableItems_Index.append(index)\n",
    "                continue\n",
    "            finally:\n",
    "                self.webdriver.quit()\n",
    "            # Dynamically add the funtions for capturing each of the product information. Take reference from \"Scrap Product info logic.py\"\n",
    "            x.addAverageRating(soup = soup_eachProduct)\n",
    "            x.addCommentCount(soup = soup_eachProduct)\n",
    "            x.addPrice(soup = soup_eachProduct)\n",
    "            x.add_Place_of_origin(soup = soup_eachProduct)\n",
    "\n",
    "        # If any product detail page cannot be accessed, filter out those products from the result dataframe accordingly\n",
    "        if len(self.ProdcutWithUntracableItems_Index) > 0:\n",
    "            self.ResultData.drop(self.ProdcutWithUntracableItems_Index, axis = 0, inplace = True)\n",
    "        # Append the product information to the result dataframe\n",
    "        for key in x.ProductData:\n",
    "            # Return error message if any of the product information is missing for the remaining products\n",
    "            assert len(x.ProductData[key])== self.ResultData.shape[0] , f\"Missing records in {key}\"\n",
    "            self.ResultData.loc[:, key] = x.ProductData[key]\n",
    "            \n",
    "# Class contain the data cleansing procedure for each of the stated columns\n",
    "class CleansingLogic:\n",
    "    def __init__(self):\n",
    "        self.columns = {\"Average Rating\" : self.Edit_AverageRating,\n",
    "                        \"Count Of Comment\" : self.Edit_CountOfComment,\n",
    "                        \"Price\" : self.Edit_Price}\n",
    "    def Edit_AverageRating(self, x):\n",
    "        return float(x)\n",
    "    \n",
    "    def Edit_CountOfComment(self, x):\n",
    "        return int(x)\n",
    "    \n",
    "    def Edit_Price(self, x):\n",
    "        return float(re.search('(\\d+\\.\\d+)', x).group())\n",
    "    \n",
    "#To automatically apply the corresponding cleansing procedure for the columns stated in the function argument\n",
    "def CleanData(df, listOfColumns):\n",
    "    Cleansed_df = df.copy()\n",
    "    for cols in listOfColumns:\n",
    "        Cleansed_df[cols] = Cleansed_df[cols].apply(lambda x : CleansingLogic().columns[cols](x))\n",
    "    return Cleansed_df\n",
    "\n",
    "# Return the data with products having the average rating within the stated range, and being sorted by the highest number of comments\n",
    "def MostCommentsWithRating(df, Rating_Low, Rating_High):\n",
    "    x = df.loc[(df['Average Rating'] >= float(Rating_Low)) & (df['Average Rating'] <= float(Rating_High))]\n",
    "    return x.sort_values(by=['Count Of Comment'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "191a43c0-524c-4e9e-afed-0c39910d780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\honso\\AppData\\Local\\Temp/ipykernel_124596/3213876960.py:33: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  self.webdriver = webdriver.Chrome(options = self.Chrome_option, executable_path = self.ChromeDriver_PATH)\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------Input Variables---------------------------------------------------\n",
    "\n",
    "#The path of the execution program of the Chrome driver program\n",
    "ChromeDriver_PATH = \"C:/Users/honso/OneDrive/Desktop/Chorme driver/Chorme driver version 104/chromedriver.exe\"\n",
    "\n",
    "#The path of the execution program of the Chrome browser\n",
    "Chrome_option = webdriver.ChromeOptions()\n",
    "Chrome_option.binary_location = r\"C:/Program Files/Google/Chrome Beta/Application/chrome.exe\"\n",
    "\n",
    "#Credentials for connecting to the local sql server\n",
    "connection_string = (\n",
    "    \"Driver={SQL Server Native Client 11.0};\"\n",
    "    \"Server=DESKTOP-A4GVUVN\\SQLEXPRESS;\"\n",
    "    \"Database={Web Scraping From HKTV Mall};\"\n",
    "    \"Trusted_Connection=yes;\"\n",
    ")\n",
    "# Prodcut category chosen for analyzing. Take reference from \"Data_URL.py\"\n",
    "Product_category = \"Cake\"\n",
    "# File name of the result dataset\n",
    "result_filename = 'Cake_20220707'\n",
    "\n",
    "#--------------------------------------------Execution----------------------------------------------------\n",
    "\n",
    "# Generate Raw Products data\n",
    "New_project = ReturnProductData(ChromeDriver_PATH = ChromeDriver_PATH, Chrome_option = Chrome_option)\n",
    "New_project.Get_productAndURL(URL().MainURL, URL().RelativeURL_By_Category[Product_category])\n",
    "New_project.AppendProductInfo()\n",
    "Raw_data = New_project.ResultData\n",
    "\n",
    "# Data Cleansing\n",
    "ColumnsToBeCleaned = ['Average Rating', 'Count Of Comment', 'Price']\n",
    "Cleaned_data = CleanData(Raw_data, ColumnsToBeCleaned)\n",
    "\n",
    "# Querying Data with customized range of product average rating, sorted by most comments given\n",
    "result = MostCommentsWithRating(Cleaned_data, Rating_Low = 4, Rating_High = 5)\n",
    "\n",
    "# Set up connection to sql server and export the result file there\n",
    "conn = connect_to_sql(connection_string)\n",
    "result.to_sql(result_filename,conn, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
